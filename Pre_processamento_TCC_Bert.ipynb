{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danosaku/classificacao_artigos_webofscience_mineracao_texto/blob/main/Pre_processamento_TCC_Bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_pretrained_bert\n",
        "!pip install transformers\n",
        "\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from keras.utils import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "## preprocessing before train bert model.\n",
        "\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from torch.optim import AdamW\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wj0NgbmLuRjr",
        "outputId": "11a410ce-4e9e-4bd7-94dd-720bcecc91aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (1.22.4)\n",
            "Collecting boto3 (from pytorch_pretrained_bert)\n",
            "  Downloading boto3-1.26.150-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (4.65.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch_pretrained_bert) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch_pretrained_bert) (16.0.5)\n",
            "Collecting botocore<1.30.0,>=1.29.150 (from boto3->pytorch_pretrained_bert)\n",
            "  Downloading botocore-1.29.150-py3-none-any.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch_pretrained_bert)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->pytorch_pretrained_bert)\n",
            "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (3.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.150->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch_pretrained_bert) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch_pretrained_bert) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.150->boto3->pytorch_pretrained_bert) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch_pretrained_bert\n",
            "Successfully installed boto3-1.26.150 botocore-1.29.150 jmespath-1.0.1 pytorch_pretrained_bert-0.6.2 s3transfer-0.6.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-JLs2pyuCS6",
        "outputId": "262c49f6-cf03-4247-d016-ca06e2531a8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "pkg-config is already the newest version (0.29.1-0ubuntu4).\n",
            "python3-dev is already the newest version (3.8.2-0ubuntu2).\n",
            "python3-dev set to manually installed.\n",
            "build-essential is already the newest version (12.8ubuntu1.1).\n",
            "The following additional packages will be installed:\n",
            "  libpoppler-cpp0v5\n",
            "The following NEW packages will be installed:\n",
            "  libpoppler-cpp-dev libpoppler-cpp0v5 poppler-utils\n",
            "0 upgraded, 3 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 220 kB of archives.\n",
            "After this operation, 977 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libpoppler-cpp0v5 amd64 0.86.1-0ubuntu1.1 [35.6 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libpoppler-cpp-dev amd64 0.86.1-0ubuntu1.1 [10.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 poppler-utils amd64 0.86.1-0ubuntu1.1 [174 kB]\n",
            "Fetched 220 kB in 0s (2,168 kB/s)\n",
            "Selecting previously unselected package libpoppler-cpp0v5:amd64.\n",
            "(Reading database ... 122541 files and directories currently installed.)\n",
            "Preparing to unpack .../libpoppler-cpp0v5_0.86.1-0ubuntu1.1_amd64.deb ...\n",
            "Unpacking libpoppler-cpp0v5:amd64 (0.86.1-0ubuntu1.1) ...\n",
            "Selecting previously unselected package libpoppler-cpp-dev:amd64.\n",
            "Preparing to unpack .../libpoppler-cpp-dev_0.86.1-0ubuntu1.1_amd64.deb ...\n",
            "Unpacking libpoppler-cpp-dev:amd64 (0.86.1-0ubuntu1.1) ...\n",
            "Selecting previously unselected package poppler-utils.\n",
            "Preparing to unpack .../poppler-utils_0.86.1-0ubuntu1.1_amd64.deb ...\n",
            "Unpacking poppler-utils (0.86.1-0ubuntu1.1) ...\n",
            "Setting up poppler-utils (0.86.1-0ubuntu1.1) ...\n",
            "Setting up libpoppler-cpp0v5:amd64 (0.86.1-0ubuntu1.1) ...\n",
            "Setting up libpoppler-cpp-dev:amd64 (0.86.1-0ubuntu1.1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import nltk\n",
        "import glob\n",
        "from nltk.stem.porter import *\n",
        "from nltk.stem import RSLPStemmer, PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "!apt install build-essential libpoppler-cpp-dev pkg-config python3-dev poppler-utils\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LnLKB6auHBp",
        "outputId": "50c54eb8-e2e7-4559-d555-04a8eb166412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "nltk.download('punkt') # punctuation\n",
        "nltk.download('stopwords')\n",
        "nltk.download('rslp')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def BERT_preprocessing(X, tokenizer):\n",
        "\n",
        "    sentences = [f\"[CLS] {i} [SEP]\" for i in X]\n",
        "\n",
        "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "    return tokenized_texts"
      ],
      "metadata": {
        "id": "ciPxSeq2tFqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--2Ce29u1_fl"
      },
      "source": [
        "Download e descompactação da planilha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbqHHt-pNYpX",
        "outputId": "1a365e26-1640-462e-bf12-b0b7f2ffadd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xDNirOca4fKFQEp1X3kDwif2lx7P6Snk\n",
            "To: /content/Dados_webofscience.zip\n",
            "\r  0% 0.00/1.36M [00:00<?, ?B/s]\r100% 1.36M/1.36M [00:00<00:00, 42.9MB/s]\n",
            "Archive:  Dados_webofscience.zip\n",
            "  inflating: Busca final 2_expressao simplificada.xls  \n"
          ]
        }
      ],
      "source": [
        "#link do arquivo no Google Drive\n",
        "#https://drive.google.com/file/d/1kFSj_Lb2XxHT1A_YO_YDRaAFtjEKTps3/view?usp=share_link\n",
        "\n",
        "\n",
        "!gdown 1xDNirOca4fKFQEp1X3kDwif2lx7P6Snk\n",
        "!unzip Dados_webofscience.zip\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmvyUzJx2JtS"
      },
      "source": [
        "Abrir planilha como um dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kN739SKCV5Y3",
        "outputId": "c7e0dfc1-1c2e-4c0f-c13a-0616646dc9d7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "    Publication Type                                            Authors  \\\n",
              "0                  J  Bertol, I; Gobbi, E; Barbosa, FT; Paz-Ferreiro...   \n",
              "1                  J  Xu, S; Silveira, ML; Sollenberger, LE; Viegas,...   \n",
              "2                  J  Lanzanova, ME; Eltz, FLF; Nicoloso, RD; Cassol...   \n",
              "3                  J  Freitas, MASR; Andrade, EM; Weber, OB; Palacio...   \n",
              "4                  J  da Costa, CFG; Figueiredo, RD; Oliveira, FD; S...   \n",
              "..               ...                                                ...   \n",
              "857                J  Moldes, CA; de Lima, OF; Merini, LJ; Tsai, SM;...   \n",
              "858                J   Silva, RD; Barioni, LG; Pellegrino, GQ; Moran, D   \n",
              "859                J  Zeid, M; Yu, JK; Goldowitz, I; Denton, ME; Cos...   \n",
              "860                J  Jesus, ED; Liang, C; Quensen, JF; Susilawati, ...   \n",
              "861                J  Eastwood, RJ; Tambam, BB; Aboagye, LM; Akparov...   \n",
              "\n",
              "     Book Authors Book Editors  Book Group Authors  \\\n",
              "0             NaN          NaN                 NaN   \n",
              "1             NaN          NaN                 NaN   \n",
              "2             NaN          NaN                 NaN   \n",
              "3             NaN          NaN                 NaN   \n",
              "4             NaN          NaN                 NaN   \n",
              "..            ...          ...                 ...   \n",
              "857           NaN          NaN                 NaN   \n",
              "858           NaN          NaN                 NaN   \n",
              "859           NaN          NaN                 NaN   \n",
              "860           NaN          NaN                 NaN   \n",
              "861           NaN          NaN                 NaN   \n",
              "\n",
              "                                     Author Full Names  \\\n",
              "0    Bertol, Ildegardis; Gobbi, Ederson; Barbosa, F...   \n",
              "1    Xu, S.; Silveira, M. L.; Sollenberger, L. E.; ...   \n",
              "2    Lanzanova, Mastrangello Enivar; Foletto Eltz, ...   \n",
              "3    Freitas, M. A. S. R.; Andrade, E. M.; Weber, O...   \n",
              "4    da Costa, Cristiane F. G.; Figueiredo, Ricardo...   \n",
              "..                                                 ...   \n",
              "857  Moldes, Carlos A.; Fontao de Lima Filho, Oscar...   \n",
              "858  Silva, Rafael De Oliveira; Barioni, Luis Gusta...   \n",
              "859  Zeid, M.; Yu, J. K.; Goldowitz, I.; Denton, M....   \n",
              "860  Jesus, Ederson da C.; Liang, Chao; Quensen, Jo...   \n",
              "861  Eastwood, Ruth J.; Tambam, Beri B.; Aboagye, L...   \n",
              "\n",
              "     Book Author Full Names  Group Authors  \\\n",
              "0                       NaN            NaN   \n",
              "1                       NaN            NaN   \n",
              "2                       NaN            NaN   \n",
              "3                       NaN            NaN   \n",
              "4                       NaN            NaN   \n",
              "..                      ...            ...   \n",
              "857                     NaN            NaN   \n",
              "858                     NaN            NaN   \n",
              "859                     NaN            NaN   \n",
              "860                     NaN            NaN   \n",
              "861                     NaN            NaN   \n",
              "\n",
              "                                         Article Title  \\\n",
              "0    WATER EROSION IN NATURAL GRASSLAND UNDER DIFFE...   \n",
              "1    Conversion of native rangelands into cultivate...   \n",
              "2    RESIDUAL EFFECT OF SOIL TILLAGE ON WATER EROSI...   \n",
              "3    Bedload sediment and nutrient losses in agro-e...   \n",
              "4    Runoff in Oxisol under different agroecosystem...   \n",
              "..                                                 ...   \n",
              "857  Occurrence of powdery mildew disease in wheat ...   \n",
              "858  The role of agricultural intensification in Br...   \n",
              "859  Cross-amplification of EST-derived markers amo...   \n",
              "860  Influence of corn, switchgrass, and prairie cr...   \n",
              "861  Adapting Agriculture to Climate Change: A Syno...   \n",
              "\n",
              "                                          Source Title  ...  \\\n",
              "0                REVISTA BRASILEIRA DE CIENCIA DO SOLO  ...   \n",
              "1               JOURNAL OF SOIL AND WATER CONSERVATION  ...   \n",
              "2                REVISTA BRASILEIRA DE CIENCIA DO SOLO  ...   \n",
              "3                   NUTRIENT CYCLING IN AGROECOSYSTEMS  ...   \n",
              "4    REVISTA BRASILEIRA DE ENGENHARIA AGRICOLA E AM...  ...   \n",
              "..                                                 ...  ...   \n",
              "857                        ACTA PHYSIOLOGIAE PLANTARUM  ...   \n",
              "858                               AGRICULTURAL SYSTEMS  ...   \n",
              "859                               FIELD CROPS RESEARCH  ...   \n",
              "860                    GLOBAL CHANGE BIOLOGY BIOENERGY  ...   \n",
              "861                                       PLANTS-BASEL  ...   \n",
              "\n",
              "      UT (Unique WOS ID)               Web of Science Record CLASS  \\\n",
              "0    WOS:000295854500036  View Full Record in Web of Science     1   \n",
              "1    WOS:000427469900010  View Full Record in Web of Science     1   \n",
              "2    WOS:000331652000025  View Full Record in Web of Science     1   \n",
              "3    WOS:000325848600005  View Full Record in Web of Science     1   \n",
              "4    WOS:000328262800007  View Full Record in Web of Science     1   \n",
              "..                   ...                                 ...   ...   \n",
              "857  WOS:000382675700006  View Full Record in Web of Science     0   \n",
              "858  WOS:000425567400009  View Full Record in Web of Science     1   \n",
              "859  WOS:000279096000004  View Full Record in Web of Science     0   \n",
              "860  WOS:000370492100019  View Full Record in Web of Science     1   \n",
              "861  WOS:000833763800001  View Full Record in Web of Science     0   \n",
              "\n",
              "    Unnamed: 73 Unnamed: 74 Unnamed: 75 Unnamed: 76 Unnamed: 77 Unnamed: 78  \\\n",
              "0           NaN         NaN         NaN         NaN         NaN         NaN   \n",
              "1           NaN         NaN         NaN         NaN         NaN         NaN   \n",
              "2           NaN         NaN         NaN         NaN         NaN         NaN   \n",
              "3           NaN         NaN         NaN         NaN         NaN         NaN   \n",
              "4           NaN         NaN         NaN         NaN         NaN         NaN   \n",
              "..          ...         ...         ...         ...         ...         ...   \n",
              "857         NaN         NaN         NaN         NaN         NaN         NaN   \n",
              "858         NaN         NaN         NaN         NaN         NaN         NaN   \n",
              "859         NaN         NaN         NaN         NaN         NaN         NaN   \n",
              "860         NaN         NaN         NaN         NaN         NaN         NaN   \n",
              "861         NaN         NaN         NaN         NaN         NaN         NaN   \n",
              "\n",
              "                               Unnamed: 79  \n",
              "0    OBS: 0 = não se aplica; 1 = se aplica  \n",
              "1                                      NaN  \n",
              "2                                      NaN  \n",
              "3                                      NaN  \n",
              "4                                      NaN  \n",
              "..                                     ...  \n",
              "857                                    NaN  \n",
              "858                                    NaN  \n",
              "859                                    NaN  \n",
              "860                                    NaN  \n",
              "861                                    NaN  \n",
              "\n",
              "[862 rows x 80 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e112ed53-516b-47ca-af71-dc8cd34b0e30\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Publication Type</th>\n",
              "      <th>Authors</th>\n",
              "      <th>Book Authors</th>\n",
              "      <th>Book Editors</th>\n",
              "      <th>Book Group Authors</th>\n",
              "      <th>Author Full Names</th>\n",
              "      <th>Book Author Full Names</th>\n",
              "      <th>Group Authors</th>\n",
              "      <th>Article Title</th>\n",
              "      <th>Source Title</th>\n",
              "      <th>...</th>\n",
              "      <th>UT (Unique WOS ID)</th>\n",
              "      <th>Web of Science Record</th>\n",
              "      <th>CLASS</th>\n",
              "      <th>Unnamed: 73</th>\n",
              "      <th>Unnamed: 74</th>\n",
              "      <th>Unnamed: 75</th>\n",
              "      <th>Unnamed: 76</th>\n",
              "      <th>Unnamed: 77</th>\n",
              "      <th>Unnamed: 78</th>\n",
              "      <th>Unnamed: 79</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>J</td>\n",
              "      <td>Bertol, I; Gobbi, E; Barbosa, FT; Paz-Ferreiro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Bertol, Ildegardis; Gobbi, Ederson; Barbosa, F...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WATER EROSION IN NATURAL GRASSLAND UNDER DIFFE...</td>\n",
              "      <td>REVISTA BRASILEIRA DE CIENCIA DO SOLO</td>\n",
              "      <td>...</td>\n",
              "      <td>WOS:000295854500036</td>\n",
              "      <td>View Full Record in Web of Science</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>OBS: 0 = não se aplica; 1 = se aplica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>J</td>\n",
              "      <td>Xu, S; Silveira, ML; Sollenberger, LE; Viegas,...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Xu, S.; Silveira, M. L.; Sollenberger, L. E.; ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Conversion of native rangelands into cultivate...</td>\n",
              "      <td>JOURNAL OF SOIL AND WATER CONSERVATION</td>\n",
              "      <td>...</td>\n",
              "      <td>WOS:000427469900010</td>\n",
              "      <td>View Full Record in Web of Science</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>J</td>\n",
              "      <td>Lanzanova, ME; Eltz, FLF; Nicoloso, RD; Cassol...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Lanzanova, Mastrangello Enivar; Foletto Eltz, ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>RESIDUAL EFFECT OF SOIL TILLAGE ON WATER EROSI...</td>\n",
              "      <td>REVISTA BRASILEIRA DE CIENCIA DO SOLO</td>\n",
              "      <td>...</td>\n",
              "      <td>WOS:000331652000025</td>\n",
              "      <td>View Full Record in Web of Science</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>J</td>\n",
              "      <td>Freitas, MASR; Andrade, EM; Weber, OB; Palacio...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Freitas, M. A. S. R.; Andrade, E. M.; Weber, O...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Bedload sediment and nutrient losses in agro-e...</td>\n",
              "      <td>NUTRIENT CYCLING IN AGROECOSYSTEMS</td>\n",
              "      <td>...</td>\n",
              "      <td>WOS:000325848600005</td>\n",
              "      <td>View Full Record in Web of Science</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>J</td>\n",
              "      <td>da Costa, CFG; Figueiredo, RD; Oliveira, FD; S...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>da Costa, Cristiane F. G.; Figueiredo, Ricardo...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Runoff in Oxisol under different agroecosystem...</td>\n",
              "      <td>REVISTA BRASILEIRA DE ENGENHARIA AGRICOLA E AM...</td>\n",
              "      <td>...</td>\n",
              "      <td>WOS:000328262800007</td>\n",
              "      <td>View Full Record in Web of Science</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>857</th>\n",
              "      <td>J</td>\n",
              "      <td>Moldes, CA; de Lima, OF; Merini, LJ; Tsai, SM;...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Moldes, Carlos A.; Fontao de Lima Filho, Oscar...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Occurrence of powdery mildew disease in wheat ...</td>\n",
              "      <td>ACTA PHYSIOLOGIAE PLANTARUM</td>\n",
              "      <td>...</td>\n",
              "      <td>WOS:000382675700006</td>\n",
              "      <td>View Full Record in Web of Science</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>858</th>\n",
              "      <td>J</td>\n",
              "      <td>Silva, RD; Barioni, LG; Pellegrino, GQ; Moran, D</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Silva, Rafael De Oliveira; Barioni, Luis Gusta...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The role of agricultural intensification in Br...</td>\n",
              "      <td>AGRICULTURAL SYSTEMS</td>\n",
              "      <td>...</td>\n",
              "      <td>WOS:000425567400009</td>\n",
              "      <td>View Full Record in Web of Science</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>859</th>\n",
              "      <td>J</td>\n",
              "      <td>Zeid, M; Yu, JK; Goldowitz, I; Denton, ME; Cos...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Zeid, M.; Yu, J. K.; Goldowitz, I.; Denton, M....</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Cross-amplification of EST-derived markers amo...</td>\n",
              "      <td>FIELD CROPS RESEARCH</td>\n",
              "      <td>...</td>\n",
              "      <td>WOS:000279096000004</td>\n",
              "      <td>View Full Record in Web of Science</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>J</td>\n",
              "      <td>Jesus, ED; Liang, C; Quensen, JF; Susilawati, ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Jesus, Ederson da C.; Liang, Chao; Quensen, Jo...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Influence of corn, switchgrass, and prairie cr...</td>\n",
              "      <td>GLOBAL CHANGE BIOLOGY BIOENERGY</td>\n",
              "      <td>...</td>\n",
              "      <td>WOS:000370492100019</td>\n",
              "      <td>View Full Record in Web of Science</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>861</th>\n",
              "      <td>J</td>\n",
              "      <td>Eastwood, RJ; Tambam, BB; Aboagye, LM; Akparov...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastwood, Ruth J.; Tambam, Beri B.; Aboagye, L...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Adapting Agriculture to Climate Change: A Syno...</td>\n",
              "      <td>PLANTS-BASEL</td>\n",
              "      <td>...</td>\n",
              "      <td>WOS:000833763800001</td>\n",
              "      <td>View Full Record in Web of Science</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>862 rows × 80 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e112ed53-516b-47ca-af71-dc8cd34b0e30')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e112ed53-516b-47ca-af71-dc8cd34b0e30 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e112ed53-516b-47ca-af71-dc8cd34b0e30');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "dados = pd.read_excel ('Busca final 2_expressao simplificada.xls')\n",
        "display(dados)\n",
        "!rm Busca\\ final\\ 2_expressao\\ simplificada.xls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScnsNw1kbxVP",
        "outputId": "6b224a1f-e89c-43b7-ff29-7ed7da6a74b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Publication Type', 'Authors', 'Book Authors', 'Book Editors',\n",
              "       'Book Group Authors', 'Author Full Names', 'Book Author Full Names',\n",
              "       'Group Authors', 'Article Title', 'Source Title', 'Book Series Title',\n",
              "       'Book Series Subtitle', 'Language', 'Document Type', 'Conference Title',\n",
              "       'Conference Date', 'Conference Location', 'Conference Sponsor',\n",
              "       'Conference Host', 'Author Keywords', 'Keywords Plus', 'Abstract',\n",
              "       'Addresses', 'Affiliations', 'Reprint Addresses', 'Email Addresses',\n",
              "       'Researcher Ids', 'ORCIDs', 'Funding Orgs', 'Funding Name Preferred',\n",
              "       'Funding Text', 'Cited References', 'Cited Reference Count',\n",
              "       'Times Cited, WoS Core', 'Times Cited, All Databases',\n",
              "       '180 Day Usage Count', 'Since 2013 Usage Count', 'Publisher',\n",
              "       'Publisher City', 'Publisher Address', 'ISSN', 'eISSN', 'ISBN',\n",
              "       'Journal Abbreviation', 'Journal ISO Abbreviation', 'Publication Date',\n",
              "       'Publication Year', 'Volume', 'Issue', 'Part Number', 'Supplement',\n",
              "       'Special Issue', 'Meeting Abstract', 'Start Page', 'End Page',\n",
              "       'Article Number', 'DOI', 'DOI Link', 'Book DOI', 'Early Access Date',\n",
              "       'Number of Pages', 'WoS Categories', 'Web of Science Index',\n",
              "       'Research Areas', 'IDS Number', 'Pubmed Id', 'Open Access Designations',\n",
              "       'Highly Cited Status', 'Hot Paper Status', 'Date of Export',\n",
              "       'UT (Unique WOS ID)', 'Web of Science Record', 'CLASS', 'Unnamed: 73',\n",
              "       'Unnamed: 74', 'Unnamed: 75', 'Unnamed: 76', 'Unnamed: 77',\n",
              "       'Unnamed: 78', 'Unnamed: 79'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "dados.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwp69Vx4u6hJ"
      },
      "outputs": [],
      "source": [
        "dados = dados.dropna(subset=['Abstract', 'CLASS'])  ## Remove linhas que não contém abstract\n",
        "text = dados[['Abstract', 'Article Title', 'Author Keywords']].apply(lambda x: ' '.join(x.astype(str)), axis=1).tolist()  ## extrai o abstract, article title e authors keywors do dataframe e junta em uma única string.\n",
        "text = [i.lower() for i in text]\n",
        "ID = dados['UT (Unique WOS ID)'].values.tolist()\n",
        "#authors = dados['Authors'].values.tolist()\n",
        "#authors = [i.lower() for i in authors]\n",
        "target = dados['CLASS'].values.tolist()\n",
        "text = [i.lower() for i in text]\n",
        "#text1 = []\n",
        "#text2 = []\n",
        "#for i in text:\n",
        "#  total = len(i)/2\n",
        "#  t1 = \"\"\n",
        "#  t2 = \"\"\n",
        "#  sentences = i.split('.')\n",
        "#  for j in sentences:\n",
        "#    if len(t1) < total:\n",
        "#      t1 = t1+ j + \".\"\n",
        "#    else:\n",
        "#      t2 = t2+ j +\".\"\n",
        "#  text1.append(t1)\n",
        "#  text2.append(t2)\n",
        "#text = text2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBIuawnMhUIu"
      },
      "source": [
        "### Gera uma lista de sentenças e de classe alvo - Não vai ser usado por enquanto ( Usando em redes heterogêneas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjhlzDqIhSZS"
      },
      "outputs": [],
      "source": [
        "sentences = []\n",
        "target_sentences = []\n",
        "for i,j in zip(text, target):\n",
        "  sub_sentences = i.split('.')\n",
        "  sentences = sentences + sub_sentences\n",
        "  for k in range(len(sub_sentences)):\n",
        "    target_sentences.append(j)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBYizZPaw7mO"
      },
      "source": [
        "### Pré-processamento de texto (remoção de stopword e stemming)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqRxpS3PueaQ"
      },
      "outputs": [],
      "source": [
        "#Cria a função remove_stopwords para tokenizar o texto e remover stopwords e outras palavras desnecessárias\n",
        "\n",
        "def remove_stopwords(text,stop_words):\n",
        "\n",
        "  ##cria lista de palavras desnecessárias (se identificar alguma no decorrer do trabalho)\n",
        "\n",
        "  unnecessary_words = [\"c\",\"annals\",\"botany\",\"company\",\"elsevier\",\"science\",\"b\",\"v\",\"ltd\",\"published\",\"all\",\n",
        "                       \"rights\",\"reserved\",\"sas\",\"gmbh\",\"authors\",\"social\",\"chemical\",\"industry\",\"saab\",\"friends\",\n",
        "                       \"keai\",\"communications\",\"co\",\"copyright\",\"john\",\"wiley\",\"sons\",\"sal\"]\n",
        "  unnecessary_words = []\n",
        "  # Adiciona palavras desnecessárias às stopwords\n",
        "\n",
        "  stop_words = stop_words + unnecessary_words\n",
        "\n",
        "  # tudo para caixa baixa\n",
        "\n",
        "  s = str(text).lower()\n",
        "\n",
        "  #tokeniza o texto\n",
        "\n",
        "  tokens = word_tokenize(s)\n",
        "\n",
        "  # remove stopwords, dígitos, caracteres especiais e pontuações\n",
        "  v = [word for word in tokens if not word in stop_words and word.isalnum() and not word.isdigit()]\n",
        "\n",
        "  return v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmVvg7olw9Wt"
      },
      "outputs": [],
      "source": [
        "#Cria a função stemming para recuperar a raiz das palavras\n",
        "\n",
        "def stemming(tokens,stemmer):\n",
        "  remove_list = ['differ', 'greater', 'studi', 'aim', 'quantifi', 'total', 'ii', 'treatment', 'mm', 'h', 'plot', 'without', 'well', 'howev', 'challeng', 'particularli', 'evalu',\n",
        "                 'respons', 'consist', 'replic', 'repres', 'spp', 'occur', 'cm', 'g', 'kg', 'also', 'show', 'effect',\n",
        "                 'taken', 'ha', 'mg', 'found', 'embrapa', 'day', 'may', 'better', 'within', 'inform', 'still', 'could', 'best', 'includ', 'wherea', 'de', 'demonstr', 'approach',\n",
        "                 'across', 'main', 'number', 'conclud', 'understand', 'known', 'earli', 'overal', 'cv', 'sp', 'due', 'possibl', 'involv', 'among', 'purpos',\n",
        "                 'x', 'input', 'amount', 'data', 'obtain', 'futur', 'unit', 'must', 'regard', 'allow', 'term', 'r', 'probabl', 'record', 'part',\n",
        "                 'increase', 'result', 'higher', 'two', 'three', 'high', 'respect', 'observ', 'experi', 'four', 'affect', 'influenc', 'one', 'per', 'contribut', 'provid', 'work',\n",
        "                 'similar', 'consid', 'five', 'suggest', 'success', 'method', 'limit', 'six', 'identifi', 'research', 'thu', 'therefor', 'altern', 'need', 'order', 'ratio', 'object word'\n",
        "                 'yr', 'although', 'current', 'remain', 'would', 'except', 'matter som', 'eight', 'discuss', 'seven', 'along', 'e', 'introduc']\n",
        "                 #,'around', 'becom', 'despit', 'experiment', 'explain', 'first', 'help', 'implement', 'introduct', 'knowledg', 'like', 'month', 'much', 'municip', 'open', 'option', 'paper',\n",
        "                 #'promis', 'report', 'requir', 'review', 'shift', 'subject', 'vari', 'variabl', 'variou', 'year', 'yr']## remoção para testes\n",
        "\n",
        "  remove_list = []\n",
        "\n",
        "  tokens_stems = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "  for l in range(len(remove_list)):\n",
        "     while(remove_list[l] in tokens_stems):\n",
        "       tokens_stems.remove(remove_list[l])\n",
        "\n",
        "  ### Remove palavras consecutivas iguais\n",
        "  m = 0\n",
        "  while(m < (len(tokens_stems)-1)):\n",
        "     if (tokens_stems[m]==tokens_stems[m+1]):\n",
        "         tokens_stems.pop(m+1)\n",
        "     m+=1\n",
        "\n",
        "  return tokens_stems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n5sFo61xTUy"
      },
      "outputs": [],
      "source": [
        "#cria função meu_tokenizador\n",
        "\n",
        "def meu_tokenizador(doc, stop_words=nltk.corpus.stopwords.words('english'), stemmer=PorterStemmer()):\n",
        "  tokens = remove_stopwords(doc,stop_words)\n",
        "  return stemming(tokens,stemmer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DzC3YpPD_Db"
      },
      "outputs": [],
      "source": [
        "def get_cluster_descriptors(VSM, df_documentos, cluster_id, max_terms=3):\n",
        "  df_descritors = pd.DataFrame()\n",
        "  df_descritors['word'] = VSM.get_feature_names_out()\n",
        "  df_descritors['tfidf_sum'] = VSM.transform(df_documentos[df_documentos.cluster==cluster_id]['text']).toarray().sum(axis=0)\n",
        "  df_descritors.sort_values(by='tfidf_sum',ascending=False,inplace=True)\n",
        "\n",
        "  num_docs = len(df_documentos[df_documentos.cluster==cluster_id]['text'])\n",
        "  descriptors =  df_descritors[df_descritors.tfidf_sum > 0].head(max_terms).word.to_list()\n",
        "\n",
        "  return num_docs,descriptors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5sKmmP4ElwG"
      },
      "source": [
        "As funções remove_stopwords e stemming estão sendo chamadas pela função meu_tokenizador. A função meu_tokenizador é utilizada na função para gerar os embbedings dos textos como no código abaixo. Você pode usar o CountVectorizer ou o TfidfVectorizer. O VSM.fit(texts) faz o treinamento do modelo para gerar os embbeddings. VSM.vocabulary_ mostra os n-gramas encontrados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s48df9CsEiHq"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "#ngram = (1,3) ## n-gramas de tamanho 1 (unigrama) até 3 (tri-grama)\n",
        "#VSM = TfidfVectorizer(tokenizer=meu_tokenizador, ngram_range = ngram)#,max_df = 0.95,min_df=0.01)\n",
        "#VSM = CountVectorizer(tokenizer=meu_tokenizador,ngram_range=ngram, min_df=2, max_df=0.95, max_features=700)\n",
        "#VSM.fit(text)\n",
        "#VSM.vocabulary_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBtGNrA8Qe82"
      },
      "outputs": [],
      "source": [
        "#from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "#ngram = (1,3) ## n-gramas de tamanho 1 (unigrama) até 3 (tri-grama)\n",
        "#VSM = TfidfVectorizer(tokenizer=meu_tokenizador, ngram_range = ngram)#,max_df = 0.95,min_df=0.01)\n",
        "#VSM = CountVectorizer(tokenizer=meu_tokenizador,ngram_range=ngram, min_df=2)#, max_df=0.95)#, max_features=500)\n",
        "#VSM.fit(text)\n",
        "#VSM.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9GTkzgncDem"
      },
      "source": [
        "remove ngrams irrelevantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODfUBDScbiPz"
      },
      "outputs": [],
      "source": [
        "def remove_ngrams_from_list(ngram_list):\n",
        "  irrelevant_ngrams = []\n",
        "\n",
        "\n",
        "  ngram_list = list(ngram_list)#inseri esse comando para transformar numpy array em lista, porque estava dando erro no trecho que remove os n-grams\n",
        "\n",
        "  for k in range(len(irrelevant_ngrams)):\n",
        "   if irrelevant_ngrams[k] in ngram_list:\n",
        "      ngram_list.remove(irrelevant_ngrams[k])\n",
        "\n",
        "  return ngram_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORkADCaVaPsF"
      },
      "source": [
        "Dividir o texto em conjunto de treinamento e de teste, com a proporção de 80% e 20%, respectivamente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fzh_e9mFaO2Y",
        "outputId": "8232f5fa-7e88-4226-869c-0da1e3ca23d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (642 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (603 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (611 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (528 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (576 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (672 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (771 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (518 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (560 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (549 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (612 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (637 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (640 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (530 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (633 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (554 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (520 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (556 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (570 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (607 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (599 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (646 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (670 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (588 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (535 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (534 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (519 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (605 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (618 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (560 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (609 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (605 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (626 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (546 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (607 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (555 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (762 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (526 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (517 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (593 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (532 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (523 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (514 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (573 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (871 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (542 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (575 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (702 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (533 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (572 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (822 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (571 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (540 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (564 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (595 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (583 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (513 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (600 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (1448 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (521 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (606 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (536 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (574 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (633 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (631 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (636 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (680 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (643 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (545 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (587 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (562 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (536 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (602 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (558 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (568 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (609 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (557 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (618 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (544 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (553 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (612 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (541 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (672 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (543 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (648 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (551 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (525 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (605 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (648 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (557 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (529 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (725 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (589 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (624 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (515 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (781 > 512). Running this sequence through BERT will result in indexing errors\n",
            "WARNING:pytorch_pretrained_bert.tokenization:Token indices sequence length is longer than the specified maximum  sequence length for this BERT model (731 > 512). Running this sequence through BERT will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  101  2300 14173  1999  3019 20331  2104  2367  2968  2015  1024  2300\n",
            "  1998  5800  6409  1998 25473  1010 18044  1998  2572 26387  1999  1996\n",
            " 19550  3128 24813  1999  2670  4380  2024  2109  2005  7125  1998  8351\n",
            " 15400  1012  1996  2005  4270  3749  1999  1996  3500  1011  2621  2161\n",
            "  2003  3618  2084  1996  4111  9095  1010  2061  1996  2005  4270  2187\n",
            "  2058  2003  2788  5296  1012 20787  5255  9754 10057 20435  1998  3727\n",
            "  1996  5800  6436  1010 29170  2300 14173  1012  2023  2817  6461  2000\n",
            " 24110 27351  2561  5800  1998  2300  6409  1998  1052  1010  1047  1998\n",
            " 18699  1006  1018  1007  1006  1009  1007  8417  1999  1996 19550  2300\n",
            "  2013  2019 23060 19565  2140  2007  3128 20787  1012  1999  1996 13441\n",
            "  1010  1996  3128 24813  1010  2020  1045  1007]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "tokenized_texts = BERT_preprocessing(text, tokenizer)\n",
        "MAX_LEN = 128\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "\n",
        "k = 5\n",
        "split = 0\n",
        "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=7)\n",
        "kfold = skf.split(input_ids, target)\n",
        "split_index = []\n",
        "for train, test in kfold:\n",
        "  split_index.append([train, test])\n",
        "X_train = [input_ids[i] for i in split_index[split][0]]\n",
        "X_train_masks = [attention_masks[i] for i in split_index[split][0]]\n",
        "y_train = [target[i] for i in split_index[split][0]]\n",
        "X_test =  [input_ids[i] for i in split_index[split][1]]\n",
        "X_test1 =  [text[i] for i in split_index[split][1]]\n",
        "X_test_masks =  [attention_masks[i] for i in split_index[split][1]]\n",
        "ID_test = [ID[i] for i in split_index[split][1]]\n",
        "y_test = [target[i] for i in split_index[split][1]]\n",
        "#X_train, X_test, y_train, y_test = train_test_split(text, target, train_size = 0.8, random_state=100)   ## 80% de treinamento. alterar train_size para o valor desejável. Exemplo: train_size=0.6 para 60% de treinamento\n",
        "print(X_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor(X_train)\n",
        "\n",
        "X_test = torch.tensor(X_test)\n",
        "y_train = torch.tensor(y_train)\n",
        "y_test = torch.tensor(y_test)\n",
        "X_train_masks = torch.tensor(X_train_masks)\n",
        "X_test_masks = torch.tensor(X_test_masks)\n",
        "batch_size=8\n",
        "      # Create an iterator of our data with torch DataLoader\n",
        "train_data = TensorDataset(X_train, X_train_masks, y_train)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "validation_data = TensorDataset(X_test, X_test_masks, y_test)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "prediction_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "UBWhUJnJymLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oekvFBjDaO0i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebEFJAhDTpeP"
      },
      "source": [
        "## Vetoriza as sentenças"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2h7_5KXXVoc"
      },
      "outputs": [],
      "source": [
        "def correlation(X1, lista, limiar):\n",
        "  X1[X1>0] = 1\n",
        "  dst = {}\n",
        "  lst = {}\n",
        "  for idx in range(X1.shape[1]-1):\n",
        "   if lista[idx] not in lst.keys():\n",
        "    for idx2 in range(idx+1, X1.shape[1]):\n",
        "      if lista[idx2] not in lst.keys():\n",
        "        minimo = np.linalg.norm(X1[:,idx] - X1[:,idx2])\n",
        "        if minimo <= limiar:\n",
        "          lst[lista[idx2]] = 0\n",
        "\n",
        "  return lst\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvCZQEIrkzEz"
      },
      "source": [
        "Este trecho é para alterar o vocabulário, ou seja, remove os n-gramas que são considerados irrelevantes para o problema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwe6OhDI7Cye"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr_LH0ufj_IP"
      },
      "source": [
        "## Imprimi a lista com os ngrams e sua frequencia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6YLaIdPfYoH"
      },
      "source": [
        "### Networkx e Plotly\n",
        "* Construção de Redes k-NN\n",
        "* Visualização Interativa de Grafos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGAGPdQT56tD"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "import networkx as nx\n",
        "from networkx.algorithms import community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzD24Igm_Diu"
      },
      "source": [
        "### Sklearn\n",
        "* Medidas de Similaridade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLEMyHnG_Div"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import kneighbors_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPFg8MOej4Xj"
      },
      "outputs": [],
      "source": [
        "def show_graph_cluster(G):\n",
        "  ### ARESTAS\n",
        "  edge_x = []\n",
        "  edge_y = []\n",
        "\n",
        "  # adicionando as coordenadas\n",
        "  for edge in G.edges():\n",
        "      x0, y0 = G.nodes[edge[0]]['pos']\n",
        "      x1, y1 = G.nodes[edge[1]]['pos']\n",
        "      edge_x.append(x0)\n",
        "      edge_x.append(x1)\n",
        "      edge_x.append(None)\n",
        "      edge_y.append(y0)\n",
        "      edge_y.append(y1)\n",
        "      edge_y.append(None)\n",
        "\n",
        "  # definindo cor e estilo das arestas\n",
        "  edge_trace = go.Scatter(\n",
        "      x=edge_x, y=edge_y,\n",
        "      line=dict(width=2, color='#888'),\n",
        "      hoverinfo='none',\n",
        "      mode='lines')\n",
        "\n",
        "  ### VÉRTICES\n",
        "  node_x = []\n",
        "  node_y = []\n",
        "\n",
        "  # adicionando as coordenadas\n",
        "  for node in G.nodes():\n",
        "      x, y = G.nodes[node]['pos']\n",
        "      node_x.append(x)\n",
        "      node_y.append(y)\n",
        "\n",
        "  # definindo cor e estilo dos vértices\n",
        "  node_trace = go.Scatter(\n",
        "      x=node_x, y=node_y,\n",
        "      mode='markers',\n",
        "      hoverinfo='text',\n",
        "      marker=dict(\n",
        "          size=10,\n",
        "          line_width=2))\n",
        "\n",
        "\n",
        "  # adicionando texto nos vértices\n",
        "  node_text = []\n",
        "  for node in G.nodes():\n",
        "      node_text.append(G.nodes[node]['text'])\n",
        "  node_trace.text = node_text\n",
        "\n",
        "  # adicionando cores nos vértices de acordo com o cluster\n",
        "  node_labels = []\n",
        "  for node in G.nodes():\n",
        "    node_labels.append(G.nodes[node]['cluster'])\n",
        "\n",
        "  node_trace.marker.color = node_labels\n",
        "\n",
        "  # visualizando!\n",
        "  fig = go.Figure(data=[edge_trace, node_trace],\n",
        "              layout=go.Layout(\n",
        "                  showlegend=False,\n",
        "                  hovermode='closest',\n",
        "                  margin=dict(b=20,l=5,r=5,t=40),\n",
        "                  xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "                  yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
        "                  )\n",
        "  fig.show()\n",
        "\n",
        "\n",
        "def show_graph_regularization(G):\n",
        "  ### ARESTAS\n",
        "  edge_x = []\n",
        "  edge_y = []\n",
        "\n",
        "  # adicionando as coordenadas\n",
        "  for edge in G.edges():\n",
        "      x0, y0 = G.nodes[edge[0]]['pos']\n",
        "      x1, y1 = G.nodes[edge[1]]['pos']\n",
        "      edge_x.append(x0)\n",
        "      edge_x.append(x1)\n",
        "      edge_x.append(None)\n",
        "      edge_y.append(y0)\n",
        "      edge_y.append(y1)\n",
        "      edge_y.append(None)\n",
        "\n",
        "  # definindo cor e estilo das arestas\n",
        "  edge_trace = go.Scatter(\n",
        "      x=edge_x, y=edge_y,\n",
        "      line=dict(width=1, color='#888'),\n",
        "      hoverinfo='none',\n",
        "      mode='lines')\n",
        "\n",
        "  ### VÉRTICES\n",
        "  node_x = []\n",
        "  node_y = []\n",
        "\n",
        "  # adicionando as coordenadas\n",
        "  for node in G.nodes():\n",
        "      x, y = G.nodes[node]['pos']\n",
        "      node_x.append(x)\n",
        "      node_y.append(y)\n",
        "\n",
        "  # definindo cor e estilo dos vértices\n",
        "  node_trace = go.Scatter(\n",
        "      x=node_x, y=node_y,\n",
        "      mode='markers',\n",
        "      hoverinfo='text',\n",
        "      marker=dict(\n",
        "          showscale=True,\n",
        "          colorscale='Reds',\n",
        "          size=10,\n",
        "          line_width=2))\n",
        "\n",
        "  # adicionando texto nos vértices\n",
        "  node_text = []\n",
        "  for node in G.nodes():\n",
        "      node_text.append(str(G.nodes[node]['f'])+'<br>'+G.nodes[node]['text'])\n",
        "  node_trace.text = node_text\n",
        "\n",
        "  # adicionando cores nos vértices de acordo com o label\n",
        "  node_labels = []\n",
        "  for node in G.nodes():\n",
        "    node_labels.append(G.nodes[node]['f'][0])\n",
        "\n",
        "  node_trace.marker.color = node_labels\n",
        "\n",
        "  # visualizando!\n",
        "  fig = go.Figure(data=[edge_trace, node_trace],\n",
        "              layout=go.Layout(\n",
        "                  showlegend=False,\n",
        "                  hovermode='closest',\n",
        "                  margin=dict(b=20,l=5,r=5,t=40),\n",
        "                  xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "                  yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
        "                  )\n",
        "  fig.show()\n",
        "\n",
        "\n",
        "def simple_regularization(G,labels,max_iter=30):\n",
        "\n",
        "  # inicializando\n",
        "  for n in G.nodes():\n",
        "    G.nodes[n]['f'] = np.array([0.0])\n",
        "    if n in labels:\n",
        "      G.nodes[n]['y'] = np.array([1.0])\n",
        "      G.nodes[n]['f'] = np.array([1.0])\n",
        "\n",
        "  for i in range(0,max_iter):\n",
        "\n",
        "    # propagando\n",
        "    diff = 0\n",
        "    for node in G.nodes():\n",
        "      if node in labels: continue\n",
        "      f_new = np.array([0.0])\n",
        "      count = 0\n",
        "      for neighbor in G.neighbors(node):\n",
        "        f_new += G.nodes[neighbor]['f']\n",
        "        count += 1\n",
        "\n",
        "      f_new /= count\n",
        "      diff += np.linalg.norm(G.nodes[node]['f']-f_new)\n",
        "      G.nodes[node]['f']=f_new\n",
        "\n",
        "      if 'y' in G.nodes[node]:\n",
        "        G.nodes[node]['f'] = G.nodes[node]['y']\n",
        "    print(\"Iteration #\"+str(i+1)+\" Q(F)=\"+str(diff))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fftb7JxT0aeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "UUSS28TxN4l7",
        "outputId": "19d37259-bfb1-4222-a383-159fe6693395"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      UT (Unique WOS ID)                                               text  \\\n",
              "0    WOS:000295854500036  water erosion in natural grassland under diffe...   \n",
              "1    WOS:000427469900010  soil organic carbon (soc) plays a critical rol...   \n",
              "2    WOS:000331652000025  soil erosion is one of the chief causes of agr...   \n",
              "3    WOS:000325848600005  the objective of this study was to quantify se...   \n",
              "4    WOS:000328262800007  in the watershed of the timboteua and buiuna s...   \n",
              "..                   ...                                                ...   \n",
              "851  WOS:000382675700006  blumeria graminis (bgt) is a pathogenic fungus...   \n",
              "852  WOS:000425567400009  brazil is the first developing country to prov...   \n",
              "853  WOS:000279096000004  the availability of a large number of expresse...   \n",
              "854  WOS:000370492100019  because soil microbes drive many of the proces...   \n",
              "855  WOS:000833763800001  the adapting agriculture to climate change pro...   \n",
              "\n",
              "     class  \n",
              "0        1  \n",
              "1        1  \n",
              "2        1  \n",
              "3        1  \n",
              "4        1  \n",
              "..     ...  \n",
              "851      0  \n",
              "852      1  \n",
              "853      0  \n",
              "854      1  \n",
              "855      0  \n",
              "\n",
              "[856 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a0404b8a-5818-48f2-861f-6f1f7d943b48\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UT (Unique WOS ID)</th>\n",
              "      <th>text</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>WOS:000295854500036</td>\n",
              "      <td>water erosion in natural grassland under diffe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>WOS:000427469900010</td>\n",
              "      <td>soil organic carbon (soc) plays a critical rol...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>WOS:000331652000025</td>\n",
              "      <td>soil erosion is one of the chief causes of agr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>WOS:000325848600005</td>\n",
              "      <td>the objective of this study was to quantify se...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>WOS:000328262800007</td>\n",
              "      <td>in the watershed of the timboteua and buiuna s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>851</th>\n",
              "      <td>WOS:000382675700006</td>\n",
              "      <td>blumeria graminis (bgt) is a pathogenic fungus...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>852</th>\n",
              "      <td>WOS:000425567400009</td>\n",
              "      <td>brazil is the first developing country to prov...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>853</th>\n",
              "      <td>WOS:000279096000004</td>\n",
              "      <td>the availability of a large number of expresse...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>854</th>\n",
              "      <td>WOS:000370492100019</td>\n",
              "      <td>because soil microbes drive many of the proces...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>855</th>\n",
              "      <td>WOS:000833763800001</td>\n",
              "      <td>the adapting agriculture to climate change pro...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>856 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a0404b8a-5818-48f2-861f-6f1f7d943b48')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a0404b8a-5818-48f2-861f-6f1f7d943b48 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a0404b8a-5818-48f2-861f-6f1f7d943b48');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "df_document = pd.DataFrame(list(zip(ID, text, target)), columns=('UT (Unique WOS ID)', 'text', 'class'))\n",
        "#df_document = pd.DataFrame(list(zip(ID, text, authors, X_gram, target)), columns=('UT (Unique WOS ID)', 'text', 'Authors', 'ngrams', 'class'))#tentando incluir authors na análise... mas deve precisar de algum pré-processamento antes\n",
        "df_document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XRThAYP_jcL"
      },
      "source": [
        "# Rede k-NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhEZSAJ_A-v9"
      },
      "source": [
        "### Gerando uma rede usando apenas documentos dos maiores clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFugyoWAAkX5"
      },
      "outputs": [],
      "source": [
        "A = kneighbors_graph(input_ids, n_neighbors=5, metric=\"cosine\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTmwLAVpAob_"
      },
      "outputs": [],
      "source": [
        "G = nx.Graph(A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "8T98HL6OBKj8",
        "outputId": "5ad2d014-87cf-4c9e-9290-a76d6af8f221"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      UT (Unique WOS ID)                                               text  \\\n",
              "0    WOS:000295854500036  water erosion in natural grassland under diffe...   \n",
              "1    WOS:000427469900010  soil organic carbon (soc) plays a critical rol...   \n",
              "2    WOS:000331652000025  soil erosion is one of the chief causes of agr...   \n",
              "3    WOS:000325848600005  the objective of this study was to quantify se...   \n",
              "4    WOS:000328262800007  in the watershed of the timboteua and buiuna s...   \n",
              "..                   ...                                                ...   \n",
              "851  WOS:000382675700006  blumeria graminis (bgt) is a pathogenic fungus...   \n",
              "852  WOS:000425567400009  brazil is the first developing country to prov...   \n",
              "853  WOS:000279096000004  the availability of a large number of expresse...   \n",
              "854  WOS:000370492100019  because soil microbes drive many of the proces...   \n",
              "855  WOS:000833763800001  the adapting agriculture to climate change pro...   \n",
              "\n",
              "     class  cluster  \n",
              "0        1        0  \n",
              "1        1        0  \n",
              "2        1        0  \n",
              "3        1        0  \n",
              "4        1        0  \n",
              "..     ...      ...  \n",
              "851      0        0  \n",
              "852      1        0  \n",
              "853      0        0  \n",
              "854      1        0  \n",
              "855      0        0  \n",
              "\n",
              "[856 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6d45402d-6f50-4688-abca-e390149259a4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UT (Unique WOS ID)</th>\n",
              "      <th>text</th>\n",
              "      <th>class</th>\n",
              "      <th>cluster</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>WOS:000295854500036</td>\n",
              "      <td>water erosion in natural grassland under diffe...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>WOS:000427469900010</td>\n",
              "      <td>soil organic carbon (soc) plays a critical rol...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>WOS:000331652000025</td>\n",
              "      <td>soil erosion is one of the chief causes of agr...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>WOS:000325848600005</td>\n",
              "      <td>the objective of this study was to quantify se...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>WOS:000328262800007</td>\n",
              "      <td>in the watershed of the timboteua and buiuna s...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>851</th>\n",
              "      <td>WOS:000382675700006</td>\n",
              "      <td>blumeria graminis (bgt) is a pathogenic fungus...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>852</th>\n",
              "      <td>WOS:000425567400009</td>\n",
              "      <td>brazil is the first developing country to prov...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>853</th>\n",
              "      <td>WOS:000279096000004</td>\n",
              "      <td>the availability of a large number of expresse...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>854</th>\n",
              "      <td>WOS:000370492100019</td>\n",
              "      <td>because soil microbes drive many of the proces...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>855</th>\n",
              "      <td>WOS:000833763800001</td>\n",
              "      <td>the adapting agriculture to climate change pro...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>856 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6d45402d-6f50-4688-abca-e390149259a4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6d45402d-6f50-4688-abca-e390149259a4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6d45402d-6f50-4688-abca-e390149259a4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "cluster_id = 0\n",
        "for clusters in community.label_propagation_communities(G):\n",
        "  for doc_id in clusters:\n",
        "    G.nodes[doc_id]['cluster'] = cluster_id\n",
        "  cluster_id +=1\n",
        "\n",
        "L_clusters = []\n",
        "for index,row in df_document.iterrows():\n",
        "  L_clusters.append(G.nodes[index]['cluster'])\n",
        "df_document['cluster'] = L_clusters\n",
        "df_document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaOGB-XZewl9"
      },
      "source": [
        "### Treina um modelo Bert e imprime o resultado de classificação"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n",
        "from tqdm import trange"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LGJs78Z5lKR",
        "outputId": "57d691a9-7406-4f86-d527-d6ed969b4c0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uY87wT5Nby8E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37711c55-c4df-4efd-abc5-6af9cdc23b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 407873900/407873900 [00:14<00:00, 28711714.82B/s]\n",
            "Epoch:  20%|██        | 1/5 [00:21<01:24, 21.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.6947517713835073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  40%|████      | 2/5 [00:39<00:58, 19.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.5993247538111931\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  60%|██████    | 3/5 [00:58<00:38, 19.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.5194295428173487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  80%|████████  | 4/5 [01:17<00:19, 19.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.4090825353944024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 5/5 [01:36<00:00, 19.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.247567416060456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score =  0.7441860465116279\n",
            "Cohen Kappa =  0.4877487478001895\n",
            "0.7433184099850767\n",
            "0.7441860465116278\n",
            "Classification accuracy using BERT Fine Tuning: 48.99%\n"
          ]
        }
      ],
      "source": [
        "nb_labels = 2\n",
        "from sklearn import linear_model, svm\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, cohen_kappa_score, f1_score, precision_score\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=nb_labels)\n",
        "model.to(device)\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "          {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "           'weight_decay_rate': 0.01},\n",
        "          {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "           'weight_decay_rate': 0.0}\n",
        "      ]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                     lr=2e-5,\n",
        "                     )\n",
        "                     #epsilon=1e-8,\n",
        "                     #warmup=.1)\n",
        "\n",
        "\n",
        "      # Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "\n",
        "      # Number of training epochs\n",
        "epochs = 5\n",
        "\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "\n",
        "        ## TRAINING\n",
        "\n",
        "        # Set our model to training mode\n",
        "        model.train()\n",
        "        # Tracking variables\n",
        "        tr_loss = 0\n",
        "        nb_tr_examples, nb_tr_steps = 0, 0\n",
        "        # Train the data for one epoch\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "          # Add batch to GPU\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          # Unpack the inputs from our dataloader\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          # Clear out the gradients (by default they accumulate)\n",
        "          optimizer.zero_grad()\n",
        "          # Forward pass\n",
        "          loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "          train_loss_set.append(loss.item())\n",
        "          # Backward pass\n",
        "          loss.backward()\n",
        "          # Update parameters and take a step using the computed gradient\n",
        "          optimizer.step()\n",
        "          # Update tracking variables\n",
        "          tr_loss += loss.item()\n",
        "          nb_tr_examples += b_input_ids.size(0)\n",
        "          nb_tr_steps += 1\n",
        "        print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # Put model in evaluation mode\n",
        "model.eval()\n",
        "      # Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "      # Predict\n",
        "for batch in prediction_dataloader:\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "        with torch.no_grad():\n",
        "          # Forward pass, calculate logit predictions\n",
        "          logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        # Store predictions and true labels\n",
        "        predictions.append(logits)\n",
        "        true_labels.append(label_ids)\n",
        "\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "matthews_set = []\n",
        "for j in range(len(true_labels)):\n",
        "        matthews = matthews_corrcoef(true_labels[j],\n",
        "                 np.argmax(predictions[j], axis=1).flatten())\n",
        "        matthews_set.append(matthews)\n",
        "\n",
        "      # Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "print(\"Accuracy score = \",accuracy_score(np.array(flat_true_labels), np.array(flat_predictions)))  ## acuracia  = (TP+TN)/(TP+TN+FP+FN)\n",
        "print(\"Cohen Kappa = \",cohen_kappa_score(np.array(flat_true_labels), np.array(flat_predictions)))\n",
        "print(f'Precision = {precision_score(np.array(flat_true_labels), np.array(flat_predictions))}')\n",
        "print(f1_score(np.array(flat_true_labels), np.array(flat_predictions), average='macro'))\n",
        "print(f1_score(np.array(flat_true_labels), np.array(flat_predictions), average='micro'))\n",
        "\n",
        "print('Classification accuracy using BERT Fine Tuning: {0:0.2%}'.format(matthews_corrcoef(flat_true_labels, flat_predictions)))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6qVa6V6TJ4B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6503
        },
        "outputId": "3c32d519-fe8a-48ce-9358-342890591032"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     ID                                               text  \\\n",
              "0   WOS:000403676000010  aimsto address whether seed and seedling funct...   \n",
              "1   WOS:000352085500002  vetiver grass has been used to control erosion...   \n",
              "2   WOS:000401398600036  this manuscript analyzes the land use changes ...   \n",
              "3   WOS:000734064100001  the death syndrome of marandu grass (brachiari...   \n",
              "4   WOS:000081519000022  alho-macho (sisyrinchium platense johnst.) is ...   \n",
              "5   WOS:000254023100009  along the amazonian pioneer front in the brazi...   \n",
              "6   WOS:000418214300024  caatinga forests occupy an area of around one ...   \n",
              "7   WOS:000402662500022  the present study aimed to evaluate two rotati...   \n",
              "8   WOS:000248652800009  araucaria angustifolia, also known as the para...   \n",
              "9   WOS:000264201600014  soil physical characteristics were evaluated o...   \n",
              "10  WOS:000576968700017  tropical ecosystems are undergoing unprecedent...   \n",
              "11  WOS:000303199800003  the objective of this study was to evaluate in...   \n",
              "12  WOS:000457575500008  backgrounds and aims the genus stylosanthes in...   \n",
              "13  WOS:000321519300013  pastures are a major soil cover in central bra...   \n",
              "14  WOS:000221071900003  the objective of this work was to determine th...   \n",
              "15  WOS:000565782100014  due to the large extent of degraded areas in t...   \n",
              "16  WOS:000184566200432  biomass produced by accessions of 26 species, ...   \n",
              "17  WOS:000564756600016  tropical grasses are used as forage, to produc...   \n",
              "18  WOS:A1997XZ31800009  the effect of cuttings on the tiller dynamics ...   \n",
              "19  WOS:000389913500012  the worldwide historical carbon (c) losses due...   \n",
              "20  WOS:000796480700005  silvopastoral systems are suggested to be impo...   \n",
              "21  WOS:000233301400005  1. the majority of tropical and subtropical fo...   \n",
              "22  WOS:000658704900001  'ursbrs mesclador' is a red clover cultivar, a...   \n",
              "23  WOS:000299438600033  intercropping of a forage and main grain crop ...   \n",
              "24  WOS:000524870200008  the objective of this study was to evaluate th...   \n",
              "25  WOS:000073517200015  an experiment was conducted in porto velho, ro...   \n",
              "26  WOS:000266649300002  the no-tillage planting system comes to be an ...   \n",
              "27  WOS:000307369900007  this work aimed to evaluate the following hypo...   \n",
              "28  WOS:000596861800020  this studyaimed to evaluate the physiological ...   \n",
              "29  WOS:000089862000001  this work was conducted in a native grassland ...   \n",
              "30  WOS:000385459500007  agroforestry systems with eucalyptus prevail i...   \n",
              "31  WOS:000278047900011  the objective of this work was to evaluate the...   \n",
              "32  WOS:000337762600007  fast-growing plant species are plentiful at th...   \n",
              "33  WOS:000378773700008  the atlantic forest is considered an environme...   \n",
              "34  WOS:000553568400009  purpose sugarcane vinasse is generated in larg...   \n",
              "35  WOS:000484997000006  defining the reference system for restoration ...   \n",
              "36  WOS:000371488300002  brazil typifies the land use changes happening...   \n",
              "37  WOS:000084960300009  this paper analyzes land use change in the bra...   \n",
              "38  WOS:000072853400003  this work provides information on genetic and ...   \n",
              "39  WOS:A1991HL31000027  the effect of sodseeding of lolium multiflorum...   \n",
              "40  WOS:000605125600002  purpose phosphorus (p) is often the main limit...   \n",
              "41  WOS:A1997YE09600004  given the rapid conversion of tropical forests...   \n",
              "42  WOS:000369463100012  in northeastern para, smallholder agriculture ...   \n",
              "43  WOS:000343318400023  land use changes in the amazon region strongly...   \n",
              "\n",
              "    predicted  target  \n",
              "0           1       0  \n",
              "1           0       1  \n",
              "2           1       0  \n",
              "3           0       1  \n",
              "4           0       1  \n",
              "5           0       1  \n",
              "6           0       1  \n",
              "7           0       1  \n",
              "8           1       0  \n",
              "9           0       1  \n",
              "10          1       0  \n",
              "11          0       1  \n",
              "12          1       0  \n",
              "13          0       1  \n",
              "14          1       0  \n",
              "15          0       1  \n",
              "16          1       0  \n",
              "17          1       0  \n",
              "18          0       1  \n",
              "19          0       1  \n",
              "20          0       1  \n",
              "21          1       0  \n",
              "22          0       1  \n",
              "23          0       1  \n",
              "24          0       1  \n",
              "25          0       1  \n",
              "26          1       0  \n",
              "27          1       0  \n",
              "28          1       0  \n",
              "29          0       1  \n",
              "30          0       1  \n",
              "31          1       0  \n",
              "32          0       1  \n",
              "33          1       0  \n",
              "34          1       0  \n",
              "35          1       0  \n",
              "36          0       1  \n",
              "37          1       0  \n",
              "38          0       1  \n",
              "39          0       1  \n",
              "40          0       1  \n",
              "41          1       0  \n",
              "42          0       1  \n",
              "43          0       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e64c1fb4-c987-427a-8ba1-a79a9fa559c8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>text</th>\n",
              "      <th>predicted</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>WOS:000403676000010</td>\n",
              "      <td>aimsto address whether seed and seedling funct...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>WOS:000352085500002</td>\n",
              "      <td>vetiver grass has been used to control erosion...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>WOS:000401398600036</td>\n",
              "      <td>this manuscript analyzes the land use changes ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>WOS:000734064100001</td>\n",
              "      <td>the death syndrome of marandu grass (brachiari...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>WOS:000081519000022</td>\n",
              "      <td>alho-macho (sisyrinchium platense johnst.) is ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>WOS:000254023100009</td>\n",
              "      <td>along the amazonian pioneer front in the brazi...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>WOS:000418214300024</td>\n",
              "      <td>caatinga forests occupy an area of around one ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>WOS:000402662500022</td>\n",
              "      <td>the present study aimed to evaluate two rotati...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>WOS:000248652800009</td>\n",
              "      <td>araucaria angustifolia, also known as the para...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>WOS:000264201600014</td>\n",
              "      <td>soil physical characteristics were evaluated o...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>WOS:000576968700017</td>\n",
              "      <td>tropical ecosystems are undergoing unprecedent...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>WOS:000303199800003</td>\n",
              "      <td>the objective of this study was to evaluate in...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>WOS:000457575500008</td>\n",
              "      <td>backgrounds and aims the genus stylosanthes in...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>WOS:000321519300013</td>\n",
              "      <td>pastures are a major soil cover in central bra...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>WOS:000221071900003</td>\n",
              "      <td>the objective of this work was to determine th...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>WOS:000565782100014</td>\n",
              "      <td>due to the large extent of degraded areas in t...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>WOS:000184566200432</td>\n",
              "      <td>biomass produced by accessions of 26 species, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>WOS:000564756600016</td>\n",
              "      <td>tropical grasses are used as forage, to produc...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>WOS:A1997XZ31800009</td>\n",
              "      <td>the effect of cuttings on the tiller dynamics ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>WOS:000389913500012</td>\n",
              "      <td>the worldwide historical carbon (c) losses due...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>WOS:000796480700005</td>\n",
              "      <td>silvopastoral systems are suggested to be impo...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>WOS:000233301400005</td>\n",
              "      <td>1. the majority of tropical and subtropical fo...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>WOS:000658704900001</td>\n",
              "      <td>'ursbrs mesclador' is a red clover cultivar, a...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>WOS:000299438600033</td>\n",
              "      <td>intercropping of a forage and main grain crop ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>WOS:000524870200008</td>\n",
              "      <td>the objective of this study was to evaluate th...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>WOS:000073517200015</td>\n",
              "      <td>an experiment was conducted in porto velho, ro...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>WOS:000266649300002</td>\n",
              "      <td>the no-tillage planting system comes to be an ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>WOS:000307369900007</td>\n",
              "      <td>this work aimed to evaluate the following hypo...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>WOS:000596861800020</td>\n",
              "      <td>this studyaimed to evaluate the physiological ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>WOS:000089862000001</td>\n",
              "      <td>this work was conducted in a native grassland ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>WOS:000385459500007</td>\n",
              "      <td>agroforestry systems with eucalyptus prevail i...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>WOS:000278047900011</td>\n",
              "      <td>the objective of this work was to evaluate the...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>WOS:000337762600007</td>\n",
              "      <td>fast-growing plant species are plentiful at th...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>WOS:000378773700008</td>\n",
              "      <td>the atlantic forest is considered an environme...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>WOS:000553568400009</td>\n",
              "      <td>purpose sugarcane vinasse is generated in larg...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>WOS:000484997000006</td>\n",
              "      <td>defining the reference system for restoration ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>WOS:000371488300002</td>\n",
              "      <td>brazil typifies the land use changes happening...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>WOS:000084960300009</td>\n",
              "      <td>this paper analyzes land use change in the bra...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>WOS:000072853400003</td>\n",
              "      <td>this work provides information on genetic and ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>WOS:A1991HL31000027</td>\n",
              "      <td>the effect of sodseeding of lolium multiflorum...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>WOS:000605125600002</td>\n",
              "      <td>purpose phosphorus (p) is often the main limit...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>WOS:A1997YE09600004</td>\n",
              "      <td>given the rapid conversion of tropical forests...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>WOS:000369463100012</td>\n",
              "      <td>in northeastern para, smallholder agriculture ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>WOS:000343318400023</td>\n",
              "      <td>land use changes in the amazon region strongly...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e64c1fb4-c987-427a-8ba1-a79a9fa559c8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e64c1fb4-c987-427a-8ba1-a79a9fa559c8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e64c1fb4-c987-427a-8ba1-a79a9fa559c8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "y_test = np.array(flat_true_labels)\n",
        "y_pred = np.array(flat_predictions)\n",
        "X_test1 = np.array(X_test1)\n",
        "erros = []\n",
        "for i in range(X_test1.shape[0]):\n",
        "   if y_test[i]!=y_pred[i]:\n",
        "     erros.append([ID_test[i], X_test1[i], y_pred[i], y_test[i]])\n",
        "df_erros = pd.DataFrame(erros, columns=['ID', 'text', 'predicted', 'target'])\n",
        "df_erros"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}